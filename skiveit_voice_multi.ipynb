{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "skiveit_voice_multi.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomarskt/voiceCollab/blob/main/skiveit_voice_multi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za124iWvdMsZ"
      },
      "source": [
        "# DeepVoice3: Multi-speaker text-to-speech demo\n",
        "\n",
        "In this notebook, you can try DeepVoice3-based multi-speaker text-to-speech (en) using a model trained on [VCTK dataset](http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html). The notebook is supposed to be executed on [Google colab](https://colab.research.google.com) so you don't have to setup your machines locally.\n",
        "\n",
        "**Estimated time to complete**: 5 miniutes.\n",
        "\n",
        "- Code: https://github.com/r9y9/deepvoice3_pytorch\n",
        "- Audio samples: https://r9y9.github.io/deepvoice3_pytorch/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml6wOhwqhGiI"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjindPTItq75"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kemMMs6pg9Rv"
      },
      "source": [
        "import os\n",
        "from os.path import exists, join, expanduser\n",
        "\n",
        "# Clone\n",
        "name = \"deepvoice3_pytorch\"\n",
        "if not exists(name):\n",
        "  ! git clone https://github.com/r9y9/$name\n",
        "arr = os.listdir()\n",
        "print(arr)\n",
        "retval = os.getcwd()\n",
        "print(retval)\n",
        "\n",
        "! ls -la"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H6mRHtAzmKd"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntBxf7b6DCqT"
      },
      "source": [
        "# Change working directory to the project dir \n",
        "#os.chdir(join(expanduser(\"~\"), name))\n",
        "os.chdir(join(os.getcwd(), name))\n",
        "\n",
        "# Use pytorch v0.3.1\n",
        "# skiveit - upgraded torch version to 1.7.0 for compatability\n",
        "!pip install -q torch==1.7.0\n",
        "\n",
        "# WARNING:tensorflow:The TensorFlow contrib module will not be included in TensorFlow 2.0.For more information, please see: * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md * https://github.com/tensorflow/addons * https://github.com/tensorflow/io (for I/O related ops)If you depend on functionality not listed there, please file an issue./usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:100.) return torch._C._cuda_getDeviceCount() > 0\n",
        "\n",
        "! pip install torch==1.7.0 torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6VFmDe-ideo"
      },
      "source": [
        "%pylab inline\n",
        "%tensorflow_version 1.x\n",
        "! pip install -q librosa nltk\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython\n",
        "from IPython.display import Audio\n",
        "# need this for English text processing frontend\n",
        "import nltk\n",
        "! python -m nltk.downloader cmudict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l1Gd2SStt0E"
      },
      "source": [
        "### Download a pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42Zwjr4UjNn_"
      },
      "source": [
        "checkpoint_path = \"20171222_deepvoice3_vctk108_checkpoint_step000300000.pth\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45Wrp8INj6Xu"
      },
      "source": [
        "if not exists(checkpoint_path):\n",
        "  !curl -O -L \"https://www.dropbox.com/s/uzmtzgcedyu531k/20171222_deepvoice3_vctk108_checkpoint_step000300000.pth\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbN0Kuo43G4U"
      },
      "source": [
        "### git checkout to the working commit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqu6dICruu56"
      },
      "source": [
        "# Copy preset file (json) from master\n",
        "# The preset file describes hyper parameters\n",
        "! git checkout master --quiet\n",
        "\n",
        "preset = \"./presets/deepvoice3_vctk.json\"\n",
        "! ls -la\n",
        "! cp -v $preset .\n",
        "preset = \"./deepvoice3_vctk.json\"\n",
        "\n",
        "# And then git checkout to the working commit\n",
        "# This is due to the model was trained a few months ago and it's not compatible\n",
        "# with the current master. \n",
        "! git checkout 0421749 --quiet\n",
        "# Nov 10th 9:55pm - Aruneesh // Removing the -q (quiet option) to look at what pip install is really doing\n",
        "! pip install  -e '.[train]'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSqDhmfGsiOH"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yJ90ESZiT_S"
      },
      "source": [
        "## Synthesis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUyhiJg03dj6"
      },
      "source": [
        "### Setup hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9sLuYgcnbZb"
      },
      "source": [
        "import hparams\n",
        "import json\n",
        "\n",
        "# Newly added params. Need to inject dummy values\n",
        "for dummy, v in [(\"fmin\", 0), (\"fmax\", 0), (\"rescaling\", False),\n",
        "                 (\"rescaling_max\", 0.999), \n",
        "                 (\"allow_clipping_in_normalization\", False)]:\n",
        "  if hparams.hparams.get(dummy) is None:\n",
        "    hparams.hparams.add_hparam(dummy, v)\n",
        "    \n",
        "# Load parameters from preset\n",
        "with open(preset) as f:\n",
        "  hparams.hparams.parse_json(f.read())\n",
        "\n",
        "# Tell we are using multi-speaker DeepVoice3\n",
        "hparams.hparams.builder = \"deepvoice3_multispeaker\"\n",
        "  \n",
        "# Inject frontend text processor\n",
        "import synthesis\n",
        "import train\n",
        "from deepvoice3_pytorch import frontend\n",
        "synthesis._frontend = getattr(frontend, \"en\")\n",
        "train._frontend =  getattr(frontend, \"en\")\n",
        "\n",
        "# alises\n",
        "fs = hparams.hparams.sample_rate\n",
        "hop_length = hparams.hparams.hop_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4NOldY83wG1"
      },
      "source": [
        "### Define utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRbelGLjiSfA"
      },
      "source": [
        "def tts(model, text, p=0, speaker_id=0, fast=False, figures=True):\n",
        "  from synthesis import tts as _tts\n",
        "  waveform, alignment, spectrogram, mel = _tts(model, text, p, speaker_id, fast)\n",
        "  if figures:\n",
        "      visualize(alignment, spectrogram)\n",
        "  IPython.display.display(Audio(waveform, rate=fs))\n",
        "  \n",
        "def visualize(alignment, spectrogram):\n",
        "  label_fontsize = 16\n",
        "  figure(figsize=(16,16))\n",
        "\n",
        "  subplot(2,1,1)\n",
        "  imshow(alignment.T, aspect=\"auto\", origin=\"lower\", interpolation=None)\n",
        "  xlabel(\"Decoder timestamp\", fontsize=label_fontsize)\n",
        "  ylabel(\"Encoder timestamp\", fontsize=label_fontsize)\n",
        "  colorbar()\n",
        "\n",
        "  subplot(2,1,2)\n",
        "  librosa.display.specshow(spectrogram.T, sr=fs, \n",
        "                           hop_length=hop_length, x_axis=\"time\", y_axis=\"linear\")\n",
        "  xlabel(\"Time\", fontsize=label_fontsize)\n",
        "  ylabel(\"Hz\", fontsize=label_fontsize)\n",
        "  tight_layout()\n",
        "  colorbar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2jmbSD430Ws"
      },
      "source": [
        "### Load the model checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr8pgqtYhvav"
      },
      "source": [
        "from train import build_model\n",
        "from train import restore_parts, load_checkpoint\n",
        "import importlib\n",
        "importlib.reload(train)\n",
        "\n",
        "\n",
        "print(\"HERE - 1\")\n",
        "print(\"HERE - 2\")\n",
        "model = build_model()\n",
        "model = load_checkpoint(checkpoint_path, model, None, True)\n",
        "print(\"HERE - 3\")\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWNcAWlPPiT2"
      },
      "source": [
        "# from train import build_model\n",
        "# from train import restore_parts, load_checkpoint\n",
        "\n",
        "# print(\"HERE - 1\")\n",
        "# model = build_model()\n",
        "# print(model)\n",
        "# print(\"HERE - 2\")\n",
        "# model = load_checkpoint(checkpoint_path, model, None, True)\n",
        "# print(\"HERE - 3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b27DwGZPj4u"
      },
      "source": [
        "# from train import build_model\n",
        "# from train import restore_parts, load_checkpoint\n",
        "\n",
        "# print(\"HERE - 1\")\n",
        "# model = build_model()\n",
        "# #print(model)\n",
        "# print(\"HERE - 2\")\n",
        "# model = load_checkpoint(checkpoint_path, model, None, True)\n",
        "# print(\"HERE - 3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrdkYMiSPk9d"
      },
      "source": [
        "# from train import build_model\n",
        "# from train import restore_parts, load_checkpoint\n",
        "\n",
        "# print(\"HERE - 1\")\n",
        "# model = build_model()\n",
        "# #print(model)\n",
        "# print(\"HERE - 2\")\n",
        "# model = load_checkpoint(checkpoint_path, model, None, True)\n",
        "# print(\"HERE - 3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOJ3miW63ywA"
      },
      "source": [
        "### Generate speech"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GR1XRy-ykbz_"
      },
      "source": [
        "# Try your favorite senteneces:)\n",
        "text = \"Hi Aditya good morning, this is the progress with a sample data set and sample trained model.Generative adversarial network or variational auto-encoder.Once upon a time there was a dear little girl who was loved by every one who looked at her, but most of all by her grandmother, and there was nothing that she would not have given to the child. A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module.\"\n",
        "\n",
        "N = 2\n",
        "# Nov 10th 11pm - Aruneesh / testing with only 1 speaker to start with\n",
        "# N= 108\n",
        "print(\"Synthesizing \\\"{}\\\" with {} different speakers\".format(text, N))\n",
        "for speaker_id in range(N):\n",
        "  print(speaker_id)\n",
        "  # tts(model, text, speaker_id=speaker_id, figures=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nirMEf2J5Roy"
      },
      "source": [
        "# With attention plot\n",
        "# tts(model, text, speaker_id=0, figures=True)\n",
        "\n",
        "import importlib\n",
        "importlib.reload(synthesis)\n",
        "\n",
        "def tts(model, text, p=0, speaker_id=0, fast=False, figures=True):\n",
        "  from synthesis import tts as _tts\n",
        "  waveform, alignment, spectrogram, mel = _tts(model, text, p, speaker_id, fast)\n",
        "  if figures:\n",
        "      visualize(alignment, spectrogram)\n",
        "  IPython.display.display(Audio(waveform, rate=fs))\n",
        "\n",
        "\n",
        "tts(model, text, speaker_id=0, figures=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArQspYbs5Aoo"
      },
      "source": [
        "For details, please visit https://github.com/r9y9/deepvoice3_pytorch"
      ]
    }
  ]
}